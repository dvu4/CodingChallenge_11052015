{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Second Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the Twitter hashtag graph and calculate the average degree of the graph. The graph should just be built using tweets that arrived in the last 60 seconds as compared to the timestamp of the latest tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'@KayKay121 dragged me to the library. Now I have to be productive  https://t.co/HjZR3d5QaQ'\n",
      "'6A has decided to postpone final vote until appeals are heard by executive board. What seems set: 7 regions.'\n",
      "'@i_am_sknapp Thanks for following us, Seth.'\n",
      "'@lezlielowe That one for @skimber is *literally* the only name I can take credit (blame) for. Thanks for noticingyou really are magical.'\n",
      "\"I knew my old ford couldn't run her down, she probly didn't like me anyhow \"\n",
      "\"I'm at Terminal de Integrao do Varadouro in Joo Pessoa, PB https://t.co/HOl34REL1a\"\n",
      "''\n",
      "\"@Louis_Tomlinson you don't have to but it would be great if u could please follow me I know u won't because you'll just think I'm just a fan\"\n",
      "'@thelisavidal Happy Bless Birthday To Gabrielle Union may your Day Bring you many Awesome Blessing'\n",
      "'@antisystemVova ,      '\n",
      "'How is Trump Doing in the Polls https://t.co/1euweGx6SF #Trump #Election #News'\n",
      "'Terbaik sedap tu.  https://t.co/IwomtqWQtM'\n",
      "'i stan 5 seconds of sexx'\n",
      "'Y te vas sabiendo que, tu despus vas a volverrr'\n",
      "\"No air. No water. No captain. WTF! @AmericanAir's wonderful customer service strikes again! #IfIEverGetToCabo https://t.co/3t8Rc1qkAa\"\n",
      "'@MaraLopezS '\n",
      "'@Gun100_ https://t.co/mMbdBDODsp'\n",
      "'Sory for my face  . (Tb) @ Otw nak Tidow. . https://t.co/TkZ6ONoJBg'\n",
      "'Linda ignorada'\n",
      "'Quando eu entro num lugar e t tocando Adele: sobe no meu conceito'\n",
      "'Lindo Viernessss :)'\n",
      "'they tell you im insane'\n",
      "'@IKEA complain https://t.co/GzyHJC6jMI'\n",
      "\"We're #hiring! Click to apply: SMB Analyst - https://t.co/lAy8j01BkE #BusinessMgmt #NettempsJobs #MenloPark, CA #Job #Jobs #CareerArc\"\n",
      "\"@ZtivenLozano '\"\n",
      "'I fell asleep on FaceTime last night  only me'\n",
      "'@htthemmingsx VC VOLTOU COM ESSE USER A MDS EU TO CHORANDO '\n",
      "'My wallpaper is everything omg  https://t.co/c1T5POgv3E'\n",
      "'Wish I was more excited for Halloween'\n",
      "'@taylorgraboski no'\n",
      "'\"Me violaba una pelotita\" Jajajajajajajaj'\n",
      "\"Mdr enfaite ils parlaient de physique et c'est parti en couille !\"\n",
      "'@RivertownBlues_ pero que preciosa '\n",
      "'Listening to liveband at Full Tank \\\\m/ (at @KilangBateri in Johor Bahru, Johor) https://t.co/5mYqnqW7yG'\n",
      "'@syahidaahmad37 ye ye tawuu'\n",
      "'Lol haha where are you rn? https://t.co/hgaat4A0zR'\n",
      "\"@DJCharlieBlac @theDudeofWV I have no problem with WVU cheating anyways. Every other team does. WVU doesn't and we suck ass. Time to cheat.\"\n",
      "'Teruk laaa ko ni https://t.co/T3y9l8FEpt'\n",
      "'@shopshopnumber1 1400ems?'\n",
      "\"Hey @timaspencet - what's for lunch today?  #Hangry #Rhonda\"\n",
      "'When I see your face, my whole day sucks. Hahaha! Feels like I want to vomit. '\n",
      "'#Football Card Specialist - SDPW  Tips 170 Won 88 Profit +219.10 ROI 12.89%  1 Tip from #LaLiga this evening &gt;&gt;&gt; https://t.co/tdkGo2rCKa'\n",
      "'@LaBomba_Televen #DeTerrorEnLaBomba https://t.co/ta99WslABd'\n",
      "'@Kiyoe_S919 '\n",
      "'Have you got your #pumpkin ready for #Halloween Have Fun! #HappyHalloween #happyfriday https://t.co/UIo2LeYPoZ https://t.co/vamv9XSsH4'\n",
      "'Merindukanmu.. @ Di Kota Makassar https://t.co/fXSmWx2p7L'\n",
      "'En la aldea godn recibiendo calaverita #diadelosmuertos @ Grupo ADO https://t.co/saMYICheG8'\n",
      "\"I'm at Praca Da Bandeira https://t.co/9Be0SV3Djg\"\n",
      "'feliz cumple crack, aguante la fafa, pero no t olvides q la mejor esta en arroyito, @maradona_n10'\n",
      "'Once awhile, he was amazed by how the beauty of his hometown is from https://t.co/GLrG6SszK3'\n",
      "'Pasca yoga... (with Lizza, Dea, and 4 others at OXA Salon) [pic]  https://t.co/OM2IeKl3v4'\n",
      "'I CANT WAIT TO SEE @iamtheELEPHANTE TONIGHT AT JACK!!!!!'\n",
      "'@Lucolba te dio mientras corras???? Yo a veces si fuerzo mucho al terminar me dan mareos.'\n",
      "'vou pedir a minha me para acender a lareira , sempre fico mais quente'\n",
      "'Je connais une meuf en filiere espagnole  pipo lille, elle a mis son iphone en espagnol deso mais mdr, expliquez moi svp'\n",
      "'Indice: Concert #surpriseNRJLimoges'\n",
      "' https://t.co/9LwIKA0Gp6'\n",
      "'Custom made jumpsuit designed by the Fashion Goddess Toyin Lawani and made from scratch at d Empire https://t.co/fHy2MUQuxH'\n",
      "'Fly https://t.co/yoF0IGe4q7'\n",
      "'@EZTheExplor3r wa ikit ke! Post ke kanyan haha'\n",
      "'I still really want Panera.'\n",
      "'uffff rico un masaje tantrico en pareja no te lo pierdas agendalo en Cali al 3154010334 https://t.co/mG8jR2i4Ad'\n",
      "'@saidanana nangis la kat bahu aku jgn nangis tepi katil'\n",
      "'@Ocean3200      ?'\n",
      "'preguia toma conta da minha vida'\n",
      "'@_AyyMall people wanted him gone cuz he seemed incompetent at the HC position, &amp; struggled to score offensively &amp; get players on that side.'\n",
      "'@SofiaDominguezD @JoseeFrattini estoy muerta lo juro!!!!!!!!!!!!!! Te adoro rubia'\n",
      "'@edagexu hi beklemediim yerden geldi hofff '\n",
      "'Other half of my niggas not even here pour a 4 up to keep me frm cryin'\n",
      "'Duh pusinggg (with Hasnal at @info_asdp)  https://t.co/jpvqX5F0FN'\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#election': 2,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#news': 2,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1,\n",
      " '#trump': 2}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#election': 2,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#news': 2,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1,\n",
      " '#trump': 2}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#election': 2,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#news': 2,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1,\n",
      " '#trump': 2}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#election': 2,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#news': 2,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1,\n",
      " '#trump': 2}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#election': 2,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#news': 2,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1,\n",
      " '#trump': 2}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#businessmgmt': 6,\n",
      " '#careerarc': 6,\n",
      " '#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#hiring!': 6,\n",
      " '#job': 6,\n",
      " '#jobs': 6,\n",
      " '#laliga': 1,\n",
      " '#menlopark,': 6,\n",
      " '#nettempsjobs': 6,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#hangry': 1,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3,\n",
      " '#rhonda': 1}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3}\n",
      "{'#football': 1,\n",
      " '#halloween': 3,\n",
      " '#happyfriday': 3,\n",
      " '#happyhalloween': 3,\n",
      " '#laliga': 1,\n",
      " '#pumpkin': 3}\n",
      "{'#halloween': 3, '#happyfriday': 3, '#happyhalloween': 3, '#pumpkin': 3}\n",
      "{'#halloween': 3, '#happyfriday': 3, '#happyhalloween': 3, '#pumpkin': 3}\n",
      "{'#halloween': 3, '#happyfriday': 3, '#happyhalloween': 3, '#pumpkin': 3}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# example of program that calculates the number of tweets cleaned\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import string\n",
    "import pprint\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "#get file path from command line argument in python\n",
    "#input_file=open(sys.argv[1],\"r+\")\n",
    "#output_file=open(sys.argv[2],\"w\")\n",
    "\n",
    "#input_file = open(\"../data-gen/tweets.txt\",\"r\")\n",
    "#input_file = open(\"../tweet_input/tweets.txt\",\"r\")\n",
    "input_file = open(\"../tweet_input/tweets_1.txt\",\"r\")\n",
    "\n",
    "output_file = open(\"../tweet_output/ft2.txt\", \"w\") \n",
    "\n",
    "\n",
    "# Replace non-ASCII characters with a single space\n",
    "def removeNonAscii(s): \n",
    "    if s is not None:\n",
    "        return \"\".join(filter(lambda x: ord(x)<128 , s)) \n",
    "\n",
    "def extract_hashtags(s):\n",
    "    htag = [word for word in s.split() if word[0] == \"#\" ]   \n",
    "    htag = map(lambda x: x.lower(), htag)\n",
    "    return list(set(htag))\n",
    "\n",
    "#def extract_hashtags(s):\n",
    "#    return [word for word in s.split() if word[0] == \"#\" ]   \n",
    "\n",
    "# Calculate the time difference in timestamp\n",
    "def time_diff(time1,time2):\n",
    "    d1 = datetime.strptime(time1, \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "    d2 = datetime.strptime(time2, \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "    return (d2-d1).total_seconds()\n",
    "\n",
    "\n",
    "# Convert to JSON format\n",
    "data = [json.loads(line) for line in input_file]\n",
    "\n",
    "result = []\n",
    "\n",
    "for line in data:\n",
    "    dic = {}\n",
    "    if  line.get(\"text\") is not None:\n",
    "        dic['time'] = line.get(\"created_at\")\n",
    "        \n",
    "        #content =  str(filter(None,removeNonAscii(line.get(\"text\")))).translate(string.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "        \n",
    "        # Replace non-ASCII characters with a single space\n",
    "        content =  str(filter(None,removeNonAscii(line.get(\"text\"))))\n",
    "        \n",
    "        # Replace all whitespace escape characters with a single space\n",
    "        content =  content.translate(string.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "        \n",
    "        pprint.pprint(content)\n",
    "        \n",
    "        # Extract hashtags from each tweet\n",
    "        list_hashtags = extract_hashtags(content)\n",
    "        #pprint.pprint(list_hashtags)\n",
    "        \n",
    "        \n",
    "        # Calculate the degree of each node in each tweet\n",
    "        dic1 = {}\n",
    "        for hashtag in list_hashtags:\n",
    "            dic1[hashtag] = len(list_hashtags) -1\n",
    "        dic['content'] = dic1\n",
    "    result.append(dic)\n",
    "#pprint.pprint(result)\n",
    " \n",
    "    \n",
    "avg_degree_f =[]\n",
    "for i in range(len(result)):\n",
    "    \n",
    "    # Collect the all hashtags within the 60 Second Window\n",
    "    hashtag_node = [result[i:][idx]['content'] for idx,_ in enumerate(result[i:]) \n",
    "          if  time_diff(result[i:][0]['time'], result[i:][idx]['time']) <= 60]\n",
    "    #pprint.pprint(hashtag_node)  \n",
    "\n",
    "    \n",
    "    # Calculate the degree of each node within the 60 Second Window\n",
    "    hashtag_graph= dict(reduce(operator.add, map(Counter, hashtag_node))) \n",
    "    #print type(hashtag_graph)\n",
    "    pprint.pprint(hashtag_graph) ,'\\n'\n",
    "\n",
    "    \n",
    "    \n",
    "    # Calculating the rolling average degree of tweet within the 60 Second Window, \n",
    "    # The average degree = sum of the degrees of all nodes in all graphs and \n",
    "    # dividing by the total number of nodes in all graphs.\n",
    "    if bool(hashtag_graph):\n",
    "        avg_degree = 1.0*sum(hashtag_graph.values())/len(hashtag_graph)\n",
    "    else:\n",
    "        avg_degree = 0\n",
    "        \n",
    "    #Output this tweet with the format of  the rolling average degree \n",
    "    output_file.write('%.2f \\n' %avg_degree)\n",
    "\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "date1 = \"Sat Mar 14 18:43:19 +0000 2015\"\n",
    "date2 = \"Sat Mar 14 18:42:49 +0000 2015\"\n",
    "\n",
    "d1 = datetime.strptime(date1, \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "d2 = datetime.strptime(date2, \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "\n",
    "dif = (d1-d2).total_seconds()\n",
    "print dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.datetime(2015, 10, 29, 17, 51, 1), datetime.datetime(2015, 10, 29, 17, 51, 30), datetime.datetime(2015, 10, 29, 17, 51, 55), datetime.datetime(2015, 10, 29, 17, 51, 56), datetime.datetime(2015, 10, 29, 17, 51, 59)]\n",
      "[datetime.datetime(2015, 10, 29, 17, 51, 30), datetime.datetime(2015, 10, 29, 17, 51, 55), datetime.datetime(2015, 10, 29, 17, 51, 56), datetime.datetime(2015, 10, 29, 17, 51, 59), datetime.datetime(2015, 10, 29, 17, 52, 5)]\n",
      "[datetime.datetime(2015, 10, 29, 17, 51, 55), datetime.datetime(2015, 10, 29, 17, 51, 56), datetime.datetime(2015, 10, 29, 17, 51, 59), datetime.datetime(2015, 10, 29, 17, 52, 5)]\n",
      "[datetime.datetime(2015, 10, 29, 17, 51, 56), datetime.datetime(2015, 10, 29, 17, 51, 59), datetime.datetime(2015, 10, 29, 17, 52, 5)]\n",
      "[datetime.datetime(2015, 10, 29, 17, 51, 59), datetime.datetime(2015, 10, 29, 17, 52, 5)]\n",
      "[datetime.datetime(2015, 10, 29, 17, 52, 5)]\n"
     ]
    }
   ],
   "source": [
    "time_test = ['Thu Oct 29 17:51:01 +0000 2015',\n",
    "        'Thu Oct 29 17:51:30 +0000 2015',\n",
    "        'Thu Oct 29 17:51:55 +0000 2015',\n",
    "        'Thu Oct 29 17:51:56 +0000 2015',\n",
    "        'Thu Oct 29 17:51:59 +0000 2015',\n",
    "        'Thu Oct 29 17:52:05 +0000 2015']\n",
    "\n",
    "def time_diff(d1,d2):\n",
    "    return (datetime.strptime(d2, \"%a %b %d %H:%M:%S +0000 %Y\") -\n",
    "            datetime.strptime(d1, \"%a %b %d %H:%M:%S +0000 %Y\")).total_seconds()\n",
    "\n",
    "def time_seconds(d):\n",
    "    return datetime.strptime(d, \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "\n",
    "\n",
    "time_list = [time_seconds(i) for i in time_test]\n",
    "#print time_list\n",
    "#print (time_list[1] - time_list[0]).total_seconds()\n",
    "th=60\n",
    "\n",
    "for i in range(len(time_list)):\n",
    "    print [time_list[i:][idx]  for idx, _ in enumerate(time_list[i:]) if (time_list[i:][idx] - time_list[i:][0]).total_seconds() <= th]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'#careerarc': 6, '#happyfriday': 3, '#news': 2, '#laliga': 1, '#jobs': 6, '#election': 2, '#job': 6, '#happyhalloween': 3, '#businessmgmt': 6, '#hiring!': 6, '#trump': 2, '#rhonda': 1, '#halloween': 3, '#nettempsjobs': 6, '#menlopark,': 6, '#hangry': 1, '#pumpkin': 3, '#football': 1}\n",
      "{'#careerarc': 6, '#happyfriday': 3, '#news': 2, '#laliga': 1, '#jobs': 6, '#election': 2, '#job': 6, '#happyhalloween': 3, '#businessmgmt': 6, '#hiring!': 6, '#trump': 2, '#rhonda': 1, '#halloween': 3, '#nettempsjobs': 6, '#menlopark,': 6, '#hangry': 1, '#pumpkin': 3, '#football': 1}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "for d in c: \n",
    "    counter.update(d)\n",
    "print counter\n",
    "d = dict(counter)\n",
    "\n",
    "print d\n",
    "'''\n",
    "from collections import Counter\n",
    "import operator\n",
    "'''\n",
    "d = dict(reduce(operator.add, map(Counter, c)))\n",
    "print d\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "test = [{'content': {u'#CareerArc': 6, u'#Jobs': 6, u'#NettempsJobs': 6, u'#MenloPark,': 6, u'#Job': 6, u'#BusinessMgmt': 6, u'#hiring!': 6}, \n",
    "        'time': u'Fri Oct 30 15:29:45 +0000 2015'},\n",
    "       {'content': {u'#Jobs': 16, u'#NettempsJobs': 1, u'#MenloPark,': 3, u'#Job': 6, u'#hiring!': 1}, \n",
    "        'time': u'Fri Oct 30 15:32:05 +0000 2015'},\n",
    "       {'content': {u'#Jobs': 5, u'#NettempsJobs': 6, u'#Job': 1, u'#BusinessMgmt': 9, u'#hiring!': 4}, \n",
    "        'time': u'Fri Oct 30 15:39:19 +0000 2015'}]\n",
    "\n",
    "#t = [i['content'] for i in test]\n",
    "t =[ i['content'] for i in result]\n",
    "d = dict(reduce(operator.add, map(Counter, t)))\n",
    "print d\n",
    "\n",
    "print dict(reduce(operator.add, map(Counter, [ i['content'] for i in result])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "average degree is: 3.56\n"
     ]
    }
   ],
   "source": [
    "print len(d)\n",
    "avg_degree = 1.0*sum(d.values())/len(d)\n",
    "print 'average degree is: %.2f' %avg_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': [u'#pumpkin', u'#Halloween', u'#HappyHalloween', u'#happyfriday'], 'time': u'Fri Oct 30 15:29:45 +0000 2015'}\n"
     ]
    }
   ],
   "source": [
    "a ={'content': [u'#pumpkin', u'#Halloween', u'#HappyHalloween', u'#happyfriday'], 'time': u'Fri Oct 30 15:29:45 +0000 2015'}\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'#pumpkin', u'#Halloween', u'#HappyHalloween', u'#happyfriday']\n"
     ]
    }
   ],
   "source": [
    "print a['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'#pumpkin': 3, u'#happyfriday': 3, u'#Halloween': 3, u'#HappyHalloween': 3}\n"
     ]
    }
   ],
   "source": [
    "dict1 = {}\n",
    "for i in a['content']:\n",
    "    dict1[i] = len(a['content']) -1\n",
    "print dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['#Spark', '#Apache'], ['#Apache', '#Hadoop', '#Storm'], ['#Apache'], ['#Flink', '#Spark'], ['#HBase', '#Spark'], ['#Hadoop', '#Apache']]\n",
      "[{'#Apache': 1, '#Spark': 1}, {'#Storm': 2, '#Apache': 2, '#Hadoop': 2}, {'#Apache': 0}, {'#Spark': 1, '#Flink': 1}, {'#Spark': 1, '#HBase': 1}, {'#Apache': 1, '#Hadoop': 1}]\n"
     ]
    }
   ],
   "source": [
    "a= ['Spark Summit East this week! #Spark #Apache (timestamp: Thu Oct 29 17:51:01 +0000 2015)',\n",
    "    'Just saw a great post on Insight Data Engineering #Apache #Hadoop #Storm (timestamp: Thu Oct 29 17:51:30 +0000 2015)',\n",
    "    'Doing great work #Apache (timestamp: Thu Oct 29 17:51:55 +0000 2015)',\n",
    "    'Excellent post on #Flink and #Spark (timestamp: Thu Oct 29 17:51:56 +0000 2015)',\n",
    "    'New and improved #HBase connector for #Spark (timestamp: Thu Oct 29 17:51:59 +0000 2015)',\n",
    "    'New 2.7.1 version update for #Hadoop #Apache (timestamp: Thu Oct 29 17:52:05 +0000 2015)']\n",
    "    \n",
    "b = [extract_hashtags(i) for i in a]\n",
    "print b\n",
    "\n",
    "c=[]\n",
    "for ele in b:\n",
    "    dic1 = {}\n",
    "    for i in ele:\n",
    "        dic1[i] = len(ele) -1\n",
    "    c.append(dic1)\n",
    "print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 6, 7, 8, 9, 10, 11, 13, 22, 40]\n"
     ]
    }
   ],
   "source": [
    "s = [1,3,5,6,7,8,9,10,11,13,22,40]\n",
    "print s"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "the correct sequence of list is (with  difference of the fisrt and last element equal 5 )\n",
    "[1, 3, 5, 6]\n",
    "[3, 5, 6, 7, 8]\n",
    "[5, 6, 7, 8, 9, 10]\n",
    "[6, 7, 8, 9, 10, 11]\n",
    "[7, 8, 9, 10, 11]\n",
    "[8, 9, 10, 11, 13]\n",
    "[9,10,11,13]\n",
    "[10,11,13]\n",
    "[11]\n",
    "[13]\n",
    "[22]\n",
    "[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 6, 7, 8]\n",
      "length of new list is: 5\n",
      "Length of original list is: 12\n"
     ]
    }
   ],
   "source": [
    "th = 5\n",
    "e = []\n",
    "for idx, val in enumerate(s):\n",
    "    if s[idx] - s[1] <= th:\n",
    "        e.append(s[idx])\n",
    "        \n",
    "print e\n",
    "print 'length of new list is:',len(e)-1 \n",
    "print 'Length of original list is:',len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "print [s[0:][idx]  for idx, _ in enumerate(s[0:]) if s[0:][idx] - s[0:][0] <= th]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "print [s[1:][idx]  for idx, _ in enumerate(s[1:]) if s[1:][idx] - s[1:][0] <= th]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "print [s[2:][idx]  for idx, _ in enumerate(s[2:]) if s[2:][idx] - s[2:][0] <= th]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 7, 8, 9, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "print [s[3:][idx]  for idx, _ in enumerate(s[3:]) if s[3:][idx] - s[3:][0] <= th]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 6]\n",
      "[3, 5, 6, 7, 8]\n",
      "[5, 6, 7, 8, 9, 10]\n",
      "[6, 7, 8, 9, 10, 11]\n",
      "[7, 8, 9, 10, 11]\n",
      "[8, 9, 10, 11, 13]\n",
      "[9, 10, 11, 13]\n",
      "[10, 11, 13]\n",
      "[11, 13]\n",
      "[13]\n",
      "[22]\n",
      "[40]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(s)):\n",
    "    print [s[i:][idx]  for idx, _ in enumerate(s[i:]) if s[i:][idx] - s[i:][0] <= th]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$F(k) = \\int_{-\\infty}^{\\infty} f(x) e^{2\\pi i k} dx$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "display(Math(r'F(k) = \\int_{-\\infty}^{\\infty} f(x) e^{2\\pi i k} dx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = \"We're #hiring! Click to apply: SMB Analyst - https://t.co/lAy8j01BkE #BusinessMgmt '#hiring! #BusinessMgmt #NettempsJobs #nettempsjobs #NETTEMPSJOBS #NETTEMPSJOBS #NETTEMPSJOBS #MenloPark #Job #Jobs #CareerArc #NettempsJobs #MenloPark, CA #Job #Jobs #CareerArc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're #hiring! Click to apply: SMB Analyst - https://t.co/lAy8j01BkE #BusinessMgmt '#hiring! #BusinessMgmt #NettempsJobs #nettempsjobs #NETTEMPSJOBS #NETTEMPSJOBS #NETTEMPSJOBS #MenloPark #Job #Jobs #CareerArc #NettempsJobs #MenloPark, CA #Job #Jobs #CareerArc\n",
      "['#hiring!',\n",
      " '#BusinessMgmt',\n",
      " '#BusinessMgmt',\n",
      " '#NettempsJobs',\n",
      " '#nettempsjobs',\n",
      " '#NETTEMPSJOBS',\n",
      " '#NETTEMPSJOBS',\n",
      " '#NETTEMPSJOBS',\n",
      " '#MenloPark',\n",
      " '#Job',\n",
      " '#Jobs',\n",
      " '#CareerArc',\n",
      " '#NettempsJobs',\n",
      " '#MenloPark,',\n",
      " '#Job',\n",
      " '#Jobs',\n",
      " '#CareerArc']\n"
     ]
    }
   ],
   "source": [
    "print s\n",
    "#print extract_hashtags(s)\n",
    "s_n = extract_hashtags(s)\n",
    "pprint.pprint(s_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#hiring!', '#businessmgmt', '#businessmgmt', '#nettempsjobs', '#nettempsjobs', '#nettempsjobs', '#nettempsjobs', '#nettempsjobs', '#menlopark', '#job', '#jobs', '#careerarc', '#nettempsjobs', '#menlopark,', '#job', '#jobs', '#careerarc']\n"
     ]
    }
   ],
   "source": [
    "p = []\n",
    "for i in s_n:\n",
    "    p.append(i.lower())\n",
    "print p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#careerarc',\n",
      " '#menlopark',\n",
      " '#jobs',\n",
      " '#nettempsjobs',\n",
      " '#menlopark,',\n",
      " '#job',\n",
      " '#businessmgmt',\n",
      " '#hiring!']\n"
     ]
    }
   ],
   "source": [
    "p = map(lambda x: x.lower(), s_n)\n",
    "pprint.pprint(list(set(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
