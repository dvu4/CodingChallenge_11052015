{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insight Data Engineering - Coding Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. First feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Clean and extract the text from the raw JSON tweets that come from the Twitter Streaming API, and track the number of tweets that contain unicode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "31 tweets contained unicode. \n",
      "\n",
      "{'content': '@KayKay121 dragged me to the library. Now I have to be productive  https://t.co/HjZR3d5QaQ',\n",
      " 'time': u'Thu Oct 29 17:51:50 +0000 2015'}\n"
     ]
    }
   ],
   "source": [
    "# example of program that calculates the number of tweets cleaned\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import string\n",
    "import pprint\n",
    "\n",
    "\n",
    "#get file path from command line argument in python\n",
    "#infile=open(sys.argv[1],\"r+\")\n",
    "#outfile=open(sys.argv[2],\"w\")\n",
    "\n",
    "#input_file = open(\"../data-gen/tweets.txt\",\"r\")\n",
    "#input_file = open(\"../tweet_input/tweets.txt\",\"r\")\n",
    "input_file = open(\"../tweet_input/tweets_1.txt\",\"r\")\n",
    "\n",
    "\n",
    "# Replace non-ASCII characters with a single space,\n",
    "def removeNonAscii(s): \n",
    "    if s is not None:\n",
    "        return \"\".join(filter(lambda x: ord(x)<128, s)) \n",
    "\n",
    "#data= []\n",
    "#for line in input_file:\n",
    "#    data.append(json.loads(line))\n",
    "#print type(data)\n",
    "#print data[0]\n",
    "#pprint.pprint(data[0])\n",
    "\n",
    "# Convert to JSON format\n",
    "data = [json.loads(line) for line in input_file]\n",
    "#pprint.pprint(data[0])\n",
    "\n",
    "\n",
    "result = []\n",
    "\n",
    "\n",
    "unicode_count = 0\n",
    "\n",
    "for line in data:\n",
    "    dic = {}\n",
    "    if  line.get(\"text\") is not None:\n",
    "        dic['time'] = line.get(\"created_at\")\n",
    "\n",
    "        dic['content'] = filter(None,removeNonAscii(line.get(\"text\")))\n",
    "        \n",
    "        # Replace all whitespace escape characters with a single space\n",
    "        dic['content'] =  str(dic['content']).translate(string.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "       \n",
    "        # Track the number of tweets containing unicode\n",
    "        if len(line.get(\"text\")) > len(removeNonAscii(line.get(\"text\"))):\n",
    "            unicode_count +=1\n",
    "        result.append(dic)\n",
    "\n",
    "print '\\n',unicode_count ,\"tweets contained unicode.\", '\\n'\n",
    "\n",
    "print \"The \", '\\n'\n",
    "pprint.pprint(result[0])\n",
    "\n",
    "\n",
    "\n",
    "# extracting the information of text\" field and  \"created_at\" field, \n",
    "# then output this tweet with the format of \n",
    "# <contents of \"text\" field> (timestamp: <contents of \"created_at\" field>)\n",
    "with open(\"../tweet_output/ft1.txt\", \"w\") as output_file:\n",
    "    [output_file.write('{0} (timestamp: {1})\\n'.format(dic['content'], dic['time'])) for dic in result]\n",
    "    output_file.write('\\n{0} tweets contained unicode.'.format(unicode_count)) \n",
    "\n",
    "#outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@antisystemVova ,      \n",
      "<type 'str'>\n",
      "The slowest run took 19.18 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "1000000 loops, best of 3: 211 ns per loop\n",
      "The slowest run took 6.01 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "1000000 loops, best of 3: 834 ns per loop\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "d= result[9]\n",
    "\n",
    "s= str(d['content'])\n",
    "print s\n",
    "print type(s)\n",
    "#s.translate(string.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "\n",
    "\n",
    "# Compare performance of translate() vs regular expressions\n",
    "table = string.maketrans(\"\\n\\t\\r\", \"   \")\n",
    "%timeit s.translate(table)\n",
    "\n",
    "\n",
    "regex = re.compile(r'[\\n\\r\\t]')\n",
    "%timeit t = regex.sub(\" \", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n",
      "a b c d\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "s = \"a\\nb\\rc\\td\"\n",
    "print type(s)\n",
    "print s.translate(string.maketrans(\"\\n\\t\\r\", \"   \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ref : http://www.compjour.org/homework/opener-project-part-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
