{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 tweets contained unicode.\n"
     ]
    }
   ],
   "source": [
    "# example of program that calculates the number of tweets cleaned\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import string\n",
    "\n",
    "#get file path from command line argument in python\n",
    "#infile=open(sys.argv[1],\"r+\")\n",
    "\n",
    "#outfile=open(sys.argv[2],\"w\")\n",
    "\n",
    "#input_file = open(\"../data-gen/tweets.txt\",\"r\")\n",
    "#input_file = open(\"../tweet_input/tweets.txt\",\"r\")\n",
    "input_file = open(\"../tweet_input/tweets_1.txt\",\"r\")\n",
    "\n",
    "\n",
    "def removeNonAscii(s): \n",
    "    if s is not None:\n",
    "        return \"\".join(filter(lambda x: ord(x)<128, s)) \n",
    "\n",
    "data= []\n",
    "for line in input_file:\n",
    "    data.append(json.loads(line))\n",
    "#print type(data)\n",
    "#print data[0]\n",
    "\n",
    "result = []\n",
    "unicode_count = 0\n",
    "for line in data:\n",
    "    dic = {}\n",
    "    if  line.get(\"text\") is not None:\n",
    "        dic['time'] = line.get(\"created_at\")\n",
    "        dic['content'] = line.get(\"text\")\n",
    "        #print dic['content']\n",
    "        dic['content'] = filter(None,removeNonAscii(line.get(\"text\")))\n",
    "        dic['content'] =  str(dic['content']).translate(string.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "        #print type(dic['content'])\n",
    "        if len(line.get(\"text\")) > len(removeNonAscii(line.get(\"text\"))):\n",
    "            unicode_count +=1\n",
    "        result.append(dic)\n",
    "#print result\n",
    "print unicode_count ,\"tweets contained unicode.\"\n",
    "\n",
    "\n",
    "# extracting the information of text\" field and  \"created_at\" field, \n",
    "# then output this tweet with the format of \n",
    "# <contents of \"text\" field> (timestamp: <contents of \"created_at\" field>)\n",
    "with open(\"../tweet_output/ft1.txt\", \"w\") as output_file:\n",
    "    [output_file.write('{0} (timestamp: {1})\\n'.format(dic['content'], dic['time'])) for dic in result]\n",
    "    output_file.write('\\n{0} tweets contained unicode.'.format(unicode_count)) \n",
    "\n",
    "#outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@antisystemVova ,      \n",
      "<type 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'@antisystemVova ,      '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "d= result[9]\n",
    "\n",
    "s= str(d['content'])\n",
    "print s\n",
    "print type(s)\n",
    "s.translate(string.maketrans(\"\\n\\t\\r\", \"   \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n",
      "a b c d\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "s = \"a\\nb\\rc\\td\"\n",
    "print type(s)\n",
    "print s.translate(string.maketrans(\"\\n\\t\\r\", \"   \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
