{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insight Data Engineering - Coding Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This challenge is to implement two features:\n",
    "\n",
    "Clean and extract the text from the raw JSON tweets that come from the Twitter Streaming API, and track the number of tweets that contain unicode.\n",
    "Calculate the average degree of a vertex in a Twitter hashtag graph for the last 60 seconds, and update this each time a new tweet appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. First feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Clean and extract the text from the raw JSON tweets that come from the Twitter Streaming API, and track the number of tweets that contain unicode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a tweet's text \"clean\", we must replace all of the escape characters (e.g. \\n, \\t , \\r ,\\\",\\',  \\/ ) and remove the unicode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "31 tweets contained unicode. \n",
      "\n",
      "The  \n",
      "\n",
      "{'content': '@lezlielowe That one for @skimber is *literally* the only name I can take credit (blame) for. Thanks for noticingyou really are magical.',\n",
      " 'time': u'Thu Oct 29 18:10:49 +0000 2015'}\n"
     ]
    }
   ],
   "source": [
    "# example of program that calculates the number of tweets cleaned\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import string\n",
    "import pprint\n",
    "\n",
    "\n",
    "#input_file = open(\"../data-gen/tweets.txt\",\"r\")\n",
    "#input_file = open(\"../tweet_input/tweets.txt\",\"r\")\n",
    "input_file = open(\"../tweet_input/tweets_1.txt\",\"r\")\n",
    "\n",
    "\n",
    "def removeNonAscii(s): \n",
    "    if s is not None:\n",
    "        return \"\".join(filter(lambda x: ord(x)<128, s)) \n",
    "\n",
    "# Convert to JSON format\n",
    "data = [json.loads(line) for line in input_file]\n",
    "\n",
    "\n",
    "result = []\n",
    "\n",
    "\n",
    "unicode_count = 0\n",
    "\n",
    "for line in data:\n",
    "    dic = {}\n",
    "    if  line.get(\"text\") is not None:\n",
    "        dic['time'] = line.get(\"created_at\")\n",
    "        \n",
    "        # Replace non-ASCII characters with a single space\n",
    "        dic['content'] = filter(None,removeNonAscii(line.get(\"text\")))\n",
    "        \n",
    "        # Replace all whitespace escape characters with a single space\n",
    "        dic['content'] =  str(dic['content']).translate(string.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "       \n",
    "        # Track the number of tweets containing unicode\n",
    "        if len(line.get(\"text\")) > len(removeNonAscii(line.get(\"text\"))):\n",
    "            unicode_count +=1\n",
    "        result.append(dic)\n",
    "\n",
    "print '\\n',unicode_count ,\"tweets contained unicode.\", '\\n'\n",
    "\n",
    "print \"The \", '\\n'\n",
    "pprint.pprint(result[3])\n",
    "\n",
    "\n",
    "\n",
    "# extracting the information of text\" field and  \"created_at\" field, \n",
    "# then output this tweet with the format of \n",
    "# <contents of \"text\" field> (timestamp: <contents of \"created_at\" field>)\n",
    "with open(\"../tweet_output/ft1.txt\", \"w\") as output_file:\n",
    "    [output_file.write('{0} (timestamp: {1})\\n'.format(dic['content'], dic['time'])) for dic in result]\n",
    "    output_file.write('\\n{0} tweets contained unicode.'.format(unicode_count)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Second Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the Twitter hashtag graph and calculate the average degree of the graph. The graph should just be built using tweets that arrived in the last 60 seconds as compared to the timestamp of the latest tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Twitter hashtag graph is a graph connecting all the hashtags that have been mentioned together in a single tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract hashtag and handle case insensitive hashtag\n",
    "def extract_hashtags(s):\n",
    "    htag = [word for word in s.split() if word[0] == \"#\" ]   \n",
    "    htag = map(lambda x: x.lower(), htag)\n",
    "    return list(set(htag))\n",
    "\n",
    "# Calculate the time difference in timestamp\n",
    "def time_diff(time1,time2):\n",
    "    d1 = datetime.strptime(time1, \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "    d2 = datetime.strptime(time2, \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "    return (d2-d1).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.60 \n",
      "\n",
      "3.86 \n",
      "\n",
      "7.00 \n",
      "\n",
      "6.37 \n",
      "\n",
      "3.83 \n",
      "\n",
      "2.28 \n",
      "\n",
      "39.25 \n",
      "\n",
      "39.25 \n",
      "\n",
      "37.88 \n",
      "\n",
      "36.92 \n",
      "\n",
      "36.70 \n",
      "\n",
      "35.93 \n",
      "\n",
      "35.52 \n",
      "\n",
      "35.28 \n",
      "\n",
      "34.82 \n",
      "\n",
      "33.52 \n",
      "\n",
      "33.40 \n",
      "\n",
      "33.09 \n",
      "\n",
      "32.43 \n",
      "\n",
      "32.29 \n",
      "\n",
      "31.66 \n",
      "\n",
      "31.47 \n",
      "\n",
      "31.25 \n",
      "\n",
      "30.87 \n",
      "\n",
      "29.57 \n",
      "\n",
      "29.42 \n",
      "\n",
      "28.98 \n",
      "\n",
      "28.40 \n",
      "\n",
      "27.86 \n",
      "\n",
      "27.50 \n",
      "\n",
      "27.32 \n",
      "\n",
      "27.19 \n",
      "\n",
      "26.54 \n",
      "\n",
      "26.20 \n",
      "\n",
      "25.43 \n",
      "\n",
      "25.16 \n",
      "\n",
      "24.66 \n",
      "\n",
      "23.27 \n",
      "\n",
      "22.87 \n",
      "\n",
      "22.62 \n",
      "\n",
      "22.03 \n",
      "\n",
      "21.25 \n",
      "\n",
      "20.41 \n",
      "\n",
      "19.80 \n",
      "\n",
      "19.68 \n",
      "\n",
      "18.30 \n",
      "\n",
      "17.71 \n",
      "\n",
      "16.77 \n",
      "\n",
      "16.80 \n",
      "\n",
      "15.77 \n",
      "\n",
      "14.79 \n",
      "\n",
      "14.14 \n",
      "\n",
      "13.68 \n",
      "\n",
      "12.84 \n",
      "\n",
      "12.08 \n",
      "\n",
      "10.74 \n",
      "\n",
      "10.47 \n",
      "\n",
      "10.56 \n",
      "\n",
      "9.46 \n",
      "\n",
      "9.24 \n",
      "\n",
      "8.90 \n",
      "\n",
      "8.78 \n",
      "\n",
      "7.76 \n",
      "\n",
      "7.01 \n",
      "\n",
      "7.07 \n",
      "\n",
      "6.62 \n",
      "\n",
      "4.97 \n",
      "\n",
      "4.03 \n",
      "\n",
      "3.43 \n",
      "\n",
      "2.53 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "3.56 \n",
      "\n",
      "3.56 \n",
      "\n",
      "3.56 \n",
      "\n",
      "3.56 \n",
      "\n",
      "3.56 \n",
      "\n",
      "3.87 \n",
      "\n",
      "3.87 \n",
      "\n",
      "3.87 \n",
      "\n",
      "3.87 \n",
      "\n",
      "3.87 \n",
      "\n",
      "3.87 \n",
      "\n",
      "3.87 \n",
      "\n",
      "3.87 \n",
      "\n",
      "3.87 \n",
      "\n",
      "3.87 \n",
      "\n",
      "3.87 \n",
      "\n",
      "3.87 \n",
      "\n",
      "3.87 \n",
      "\n",
      "2.00 \n",
      "\n",
      "2.00 \n",
      "\n",
      "2.00 \n",
      "\n",
      "2.00 \n",
      "\n",
      "2.00 \n",
      "\n",
      "2.00 \n",
      "\n",
      "2.00 \n",
      "\n",
      "2.00 \n",
      "\n",
      "2.00 \n",
      "\n",
      "2.00 \n",
      "\n",
      "2.00 \n",
      "\n",
      "2.00 \n",
      "\n",
      "2.00 \n",
      "\n",
      "2.00 \n",
      "\n",
      "2.00 \n",
      "\n",
      "2.00 \n",
      "\n",
      "2.33 \n",
      "\n",
      "2.33 \n",
      "\n",
      "3.00 \n",
      "\n",
      "3.00 \n",
      "\n",
      "3.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n",
      "0.00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in data:\n",
    "    dic = {}\n",
    "    if  line.get(\"text\") is not None:\n",
    "        dic['time'] = line.get(\"created_at\")\n",
    "        \n",
    "        #content =  str(filter(None,removeNonAscii(line.get(\"text\")))).translate(string.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "        \n",
    "        # Replace non-ASCII characters with a single space\n",
    "        content =  str(filter(None,removeNonAscii(line.get(\"text\"))))\n",
    "        \n",
    "        # Replace all whitespace escape characters with a single space\n",
    "        content =  content.translate(string.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "        \n",
    "        #pprint.pprint(content)\n",
    "        \n",
    "        # Extract hashtags from each tweet\n",
    "        list_hashtags = extract_hashtags(content)\n",
    "        #pprint.pprint(list_hashtags)\n",
    "        \n",
    "        \n",
    "        # Calculate the degree of each node in each tweet\n",
    "        dic1 = {}\n",
    "        for hashtag in list_hashtags:\n",
    "            dic1[hashtag] = len(list_hashtags) -1\n",
    "        dic['content'] = dic1\n",
    "    result.append(dic)\n",
    "\n",
    " \n",
    "    \n",
    "avg_degree_f =[]\n",
    "for i in range(len(result)):\n",
    "    \n",
    "    # Collect the all hashtags within the 60 Second Window\n",
    "    hashtag_node = [result[i:][idx]['content'] for idx,_ in enumerate(result[i:]) \n",
    "          if  time_diff(result[i:][0]['time'], result[i:][idx]['time']) <= 60]\n",
    " \n",
    "\n",
    "    \n",
    "    # Calculate the degree of each node within the 60 Second Window\n",
    "    hashtag_graph= dict(reduce(operator.add, map(Counter, hashtag_node))) \n",
    "    #print type(hashtag_graph)\n",
    "    #pprint.pprint(hashtag_graph) ,'\\n'\n",
    "\n",
    "    \n",
    "    \n",
    "    # Calculating the rolling average degree of tweet within the 60 Second Window, \n",
    "    # The average degree = sum of the degrees of all nodes in all graphs and \n",
    "    # dividing by the total number of nodes in all graphs.\n",
    "    if bool(hashtag_graph):\n",
    "        avg_degree = 1.0*sum(hashtag_graph.values())/len(hashtag_graph)\n",
    "    else:\n",
    "        avg_degree = 0\n",
    "        \n",
    "    #Output this tweet with the format of  the rolling average degree \n",
    "    print '%.2f \\n' %avg_degree\n",
    "    #output_file.write('%.2f \\n' %avg_degree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
